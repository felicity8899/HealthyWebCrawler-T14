# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import pymongo

from elasticsearch import Elasticsearch, helpers
# from elasticsearch import Elasticsearch, RequestsHttpConnection, serializer, compat, exceptions, helpers
from scrapy.utils.project import get_project_settings

from scrapy.conf import settings
from scrapy.exceptions import DropItem
from scrapy import log

from datetime import datetime
import types

import pysolr
import logging


class XiaomiappPipeline(object):
    def process_item(self, item, spider):
        return item



class XiaomiElasticSearchPipeline(object):
    items_buffer = []

    def __init__(self):
        # self.settings = get_project_settings()
        self.settings = settings
        uri = "{}:{}".format(self.settings['ELASTICSEARCH_SERVER'], self.settings['ELASTICSEARCH_PORT'])
        self.es = Elasticsearch([uri])
        # uri = "%s:%d" % (self.settings['ELASTICSEARCH_SERVER'], self.settings['ELASTICSEARCH_PORT'])
        # self.es = Elasticsearch(, serializer=JSONSerializerPython2())
        # print uri

    def index_item(self, item):

        index_name = self.settings['ELASTICSEARCH_INDEX']
        index_suffix_format = self.settings.get('ELASTICSEARCH_INDEX_DATE_FORMAT', None)

        if index_suffix_format:
            index_name += "-" + datetime.strftime(datetime.now(), index_suffix_format)

        index_action = {
            '_index': index_name,
            '_type': self.settings['ELASTICSEARCH_TYPE'],
            '_source': dict(item)
        }

        self.items_buffer.append(index_action)

        # index_name = self.settings['ELASTICSEARCH_INDEX']
        # self.es.index(index_name, doc_type="test-type", body=dict(item), id=item['appid'], op_type='create')

        if len(self.items_buffer) == self.settings.get('ELASTICSEARCH_BUFFER_LENGTH', 500):
            self.send_items()
            self.items_buffer = []

    def send_items(self):
        helpers.bulk(self.es, self.items_buffer)

    def process_item(self, item, spider):
        if isinstance(item, types.GeneratorType) or isinstance(item, types.ListType):
            for each in item:
                self.process_item(each, spider)

        else:
            self.index_item(item)
        # index_name = self.settings['ELASTICSEARCH_INDEX']
        # self.es.index(dict(item), index_name, self.settings['ELASTICSEARCH_TYPE'], op_type='create')
        # self.es.index(index_name, doc_type="document", body=dict(item), op_type='create')
        self.es.index(self.settings['ELASTICSEARCH_INDEX'], self.settings['ELASTICSEARCH_TYPE'], dict(item), id=item['appid'], op_type='create', )
        # self.es.index()
        return item

    def close_spider(self, spider):
        if not self.items_buffer:
            self.send_items()

class XiaomiSolrPipeline(object):
    def __init__(self):
        self.mapping = settings['SOLR_MAPPING'].items()
        self.ignore = settings['SOLR_IGNORE_DUPLICATES'] or False
        self.keys = settings['SOLR_DUPLICATES_KEY_FIELDS']
        if self.ignore and not self.keys:
            raise RuntimeError('SOLR_DUPLICATES_KEY_FIELDS has to be defined')
        self.solr = pysolr.Solr(settings['SOLR_URL'], timeout=10)

        # print settings['SOLR_MAPPING']

    def process_item(self, item, spider):
        if self.ignore:
            duplicates = [str(name) + ':' + '"' + self.get_value(item, value) + '"' for name, value in self.mapping if name in self.keys]
            query = " ".join(duplicates)
            result = self.solr.search(query)
            print result
            # result = None
            if result:
                logging.info("Skip duplicates")
                return item
            results = {}
            for name, value in self.mapping:
                results[name] = self.get_value(item, value)

            self.solr.add([results])
            # print self.solr.search(q=query)
            # print self.solr.add([{'bananas': '1'}])
            print duplicates
            print query
            # print results
            # flag = 'appid' in item
            # print flag
            # print type(item)
            return item

    def get_value(self, item, value):
        if type(value) is str:
            return item[value] if value in item else None
        elif type(value) is list:
            return [item[key] if key in item else None for key in value]
        else:
            raise TypeError('Only string and list are valid sources')

class XiaomiMongoDBPipeline(object):
    def __init__(self):
        connection = pymongo.MongoClient(
            settings['MONGODB_SERVER'],
            settings['MONGODB_PORT']
        )
        db = connection[settings['MONGODB_DB']]
        self.collection = db[settings['MONGODB_COLLECTION']]
        self.key = settings['MONGODB_UNIQUE_KEY']

    def process_item(self, item, spider):
        valid = True
        for data in item:
            if not data:
                valid = False
                raise DropItem("Missing {0}!".format(data))
        if valid:
            # duplicate = str(self.key) + ':' + '"' + item[self.key] + '"'
            # print duplicate
            result = {}
            result[self.key] = item[self.key]
            # print result
            search_result = self.collection.find_one(result)
            # print search_result
            if search_result:
                logging.info("Skip duplicates")
                return item
            else:
                self.collection.insert(dict(item))
                log.msg("Item added to MongoDB database!",
                        level=log.DEBUG, spider=spider)
                return item

    def __get_itemvalue__(self, item, value):
        if type(value) is str:
            return item[value] if value in item else None
        elif type(value) is list:
            return [item[key] if key in item else None for key in value]
        else:
            raise TypeError('Only string and list are valid sources')


